{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from validate_detector import *\n",
    "\n",
    "\n",
    "params = dict(\n",
    "    data='LogoDet-3K_CIL.yaml',\n",
    "    yolo_model_path='weights/yolov5m6-CIL-512px-1000cls.pt',\n",
    "    cil_model_path='weights/CIL_1000_250_2993-WA-mem50-resnet34-pretrained-drop0.5-augmented-adam.pt',\n",
    "    student_model_path='weights/kd_resnet50-drop0.3-mem50_STATE_DICT.pt',\n",
    "    detection_out='tmp',\n",
    "    detection_input='dataset/LogoDet-3K_det4cil/inf',\n",
    "    conf_thres=0.4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "yolo_res_path = Path('yolov5/runs/detect/') / params['detection_out']\n",
    "\n",
    "inf_out = Path('yolov5/runs/detect/') / (params['detection_input'] + '-res')\n",
    "\n",
    "# Metadata df\n",
    "metadata = pd.read_pickle(Path(DATASET_PATH) / LOGODET_3K_NORMAL_PATH / METADATA_CROPPED_IMAGE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Configure\n",
    "cil_model, cil_idx2class, cil_class2idx, cil_class_remap = load_cil_model(\n",
    "    ROOT / params['cil_model_path'],\n",
    "    params['student_model_path']\n",
    ")\n",
    "cil_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "shutil.rmtree(yolo_res_path, ignore_errors=True)\n",
    "shutil.rmtree(str(Path(params['detection_input']+'-res')), ignore_errors=True)\n",
    "os.makedirs(str(Path(params['detection_input']+'-res')))\n",
    "\n",
    "cmd = f'python yolov5/detect.py ' \\\n",
    "      f'--data {params[\"data\"]} ' \\\n",
    "      f'--weights {params[\"yolo_model_path\"]} ' \\\n",
    "      f'--source {params[\"detection_input\"]} ' \\\n",
    "      f'--conf-thres {params[\"conf_thres\"]} ' \\\n",
    "      f'--name {params[\"detection_out\"]} ' \\\n",
    "      f'--augment --save-txt --agnostic-nms --name tmp --exist-ok'\n",
    "\n",
    "process = subprocess.Popen(cmd, shell=True)\n",
    "(output, err) = process.communicate()\n",
    "p_status = process.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inf_images = [x for x in os.listdir(params['detection_input']) if Path(x).suffix != '']\n",
    "img2label = {}\n",
    "\n",
    "print(inf_images)\n",
    "\n",
    "for file in inf_images:\n",
    "    label_path = (yolo_res_path / 'labels' / file).with_suffix('.txt')\n",
    "    if label_path.is_file():\n",
    "        with open(label_path) as f:\n",
    "            labels = [[float(y) for y in x.strip().split()][1:] for x in f.readlines()]\n",
    "    else:\n",
    "        labels = []\n",
    "    image_path = Path(params['detection_input']) / file\n",
    "    img2label.update([(image_path, labels)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def xywh2xyxy(x):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "\n",
    "bbox_color = 'green'\n",
    "font_color = (255,255,255)\n",
    "\n",
    "font_size = 18\n",
    "font = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "\n",
    "box_width = 3\n",
    "top = 10\n",
    "\n",
    "for i, el in enumerate(img2label.items()):\n",
    "    img_path, label = el\n",
    "    im = Image.open(img_path)\n",
    "    #im.show()\n",
    "\n",
    "    width, height = im.size\n",
    "    if label:\n",
    "        dim_tensor = torch.tensor([width, height, width, height]).repeat(len(label), 1)\n",
    "\n",
    "        native_pred = (xywh2xyxy(torch.tensor(label)) * dim_tensor).round()\n",
    "        im1 = im.copy()\n",
    "        im2 = im.crop()\n",
    "\n",
    "        im_bbox = ImageDraw.Draw(im1)\n",
    "        im_bbox_annotated = ImageDraw.Draw(im2)\n",
    "        for j, pred in enumerate(native_pred):\n",
    "            # Crop image\n",
    "            cropped = im.crop(np.array(pred)).convert('RGB')\n",
    "            # transform cropped image\n",
    "            common_trsf = iLogoDet3K_trsf['common']\n",
    "            test_trsf = iLogoDet3K_trsf['test']\n",
    "            all_trsf = transforms.Compose([*test_trsf, *common_trsf])\n",
    "            cropped = all_trsf(cropped)\n",
    "            # CIL model prediction\n",
    "            cil_prediction = cil_model(cropped.expand(1, *cropped.shape))\n",
    "            cil_class = cil_prediction.argmax().int().item()\n",
    "            # Predictions\n",
    "            resolved_label = cil_idx2class[cil_class_remap[cil_class]]\n",
    "            print(f'Image {i} - bbox {j}: {resolved_label}')\n",
    "            # Generate images\n",
    "            im_bbox.rectangle(np.array(pred), fill=None, outline=bbox_color, width=box_width)\n",
    "\n",
    "            im_bbox_annotated.rectangle(np.array(pred), fill=None, outline=bbox_color, width=box_width)\n",
    "            pos = np.array(pred)\n",
    "            pos = [pos[0], pos[1]-font_size, pos[2], pos[1]]\n",
    "            im_bbox_annotated.rectangle(pos, fill=bbox_color, outline=bbox_color, width=box_width)\n",
    "\n",
    "            text_pos = [pos[0]+box_width+1, pos[1]]\n",
    "            im_bbox_annotated.text(text_pos, resolved_label, font_color, font=font)\n",
    "\n",
    "        prefix = Path(params['detection_input']+'-res')\n",
    "        im1.save(prefix  / (img_path.stem + '_bbox.png'))\n",
    "        im2.save(prefix / (img_path.stem + '_bbox-clf.png'))\n",
    "    else:\n",
    "        im.save(prefix  / (img_path.stem + '_bbox.png'))\n",
    "        im.save(prefix / (img_path.stem + '_bbox-clf.png'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}